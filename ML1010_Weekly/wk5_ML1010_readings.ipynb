{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pn8fyGevt62D"
   },
   "source": [
    "# Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "XmHgvt15t48a"
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "PROJECT_NAME = 'ML1010-Group-Project'\n",
    "ENABLE_COLAB = False\n",
    "\n",
    "#Root Machine Learning Directory. Projects appear underneath\n",
    "GOOGLE_DRIVE_MOUNT = '/content/gdrive' \n",
    "COLAB_ROOT_DIR = GOOGLE_DRIVE_MOUNT + '/MyDrive/Colab Notebooks'\n",
    "COLAB_INIT_DIR = COLAB_ROOT_DIR + '/utility_files'\n",
    "\n",
    "LOCAL_ROOT_DIR = '/home/magni//ML_Root/project_root'\n",
    "LOCAL_INIT_DIR = LOCAL_ROOT_DIR + '/utility_files'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1QZKvJBJ7rlc"
   },
   "source": [
    "# Bootstrap Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2266,
     "status": "ok",
     "timestamp": 1637961005073,
     "user": {
      "displayName": "Michael Vasiliou",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "07079983815732559270"
     },
     "user_tz": 300
    },
    "id": "jgMPxKZBzfFw",
    "outputId": "764d1c27-8651-4188-e412-32d75dbc940d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wha...where am I?\n",
      "I am awake now.\n",
      "\n",
      "I have set your current working directory to /home/magni/ML_Root/project_root/ML1010-Group-Project\n",
      "The current time is 19:14\n",
      "Hello sir. I hope you had dinner.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#add in support for utility file directory and importing\n",
    "import sys\n",
    "import os\n",
    "\n",
    "if ENABLE_COLAB:\n",
    "  #Need access to drive\n",
    "  from google.colab import drive\n",
    "  drive.mount(GOOGLE_DRIVE_MOUNT, force_remount=True)\n",
    "  \n",
    "  #add in utility directory to syspath to import \n",
    "  INIT_DIR = COLAB_INIT_DIR\n",
    "  sys.path.append(os.path.abspath(INIT_DIR))\n",
    "  \n",
    "  #Config environment variables\n",
    "  ROOT_DIR = COLAB_ROOT_DIR\n",
    "  \n",
    "else:\n",
    "  #add in utility directory to syspath to import\n",
    "  INIT_DIR = LOCAL_INIT_DIR\n",
    "  sys.path.append(os.path.abspath(INIT_DIR))\n",
    "  \n",
    "  #Config environment variables\n",
    "  ROOT_DIR = LOCAL_ROOT_DIR\n",
    "\n",
    "#Import Utility Support\n",
    "from jarvis import Jarvis\n",
    "jarvis = Jarvis(ROOT_DIR, PROJECT_NAME)\n",
    "\n",
    "import mv_python_utils as mvutils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fVJrz4vi6Cy5"
   },
   "source": [
    "# Setup Runtime Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 229,
     "status": "ok",
     "timestamp": 1637802866801,
     "user": {
      "displayName": "Michael Vasiliou",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "07079983815732559270"
     },
     "user_tz": 300
    },
    "id": "CHI-rXnnyTbi",
    "outputId": "22debd2c-e9c7-42af-e7a9-bbe3c8d79d94"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Google Colab not enabled'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/magni/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "if ENABLE_COLAB:\n",
    "  #!pip install scipy -q\n",
    "  #!pip install scikit-learn -q\n",
    "  #!pip install pycaret -q\n",
    "  #!pip install matplotlib -q\n",
    "  #!pip install joblib -q\n",
    "  #!pip install pandasql -q\n",
    "\n",
    "  display('Google Colab enabled')\n",
    "else:\n",
    "  display('Google Colab not enabled')\n",
    "\n",
    "#Common imports\n",
    "import json\n",
    "import gzip\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import re\n",
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "pd.set_option('mode.chained_assignment', None)\n",
    "nltk.download('stopwords')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GIOd0sJRBosG"
   },
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "PBeX0D7GMTuq"
   },
   "outputs": [],
   "source": [
    "# From article https://machinelearningmastery.com/sequence-classification-lstm-recurrent-neural-networks-python-keras/\n",
    "\n",
    "import numpy\n",
    "from keras.datasets import imdb\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "\n",
    "# fix random seed for reproducibility\n",
    "numpy.random.seed(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the dataset but only keep the top n words, zero the rest\n",
    "top_words = 5000\n",
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000, 500)\n",
      "1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>490</th>\n",
       "      <th>491</th>\n",
       "      <th>492</th>\n",
       "      <th>493</th>\n",
       "      <th>494</th>\n",
       "      <th>495</th>\n",
       "      <th>496</th>\n",
       "      <th>497</th>\n",
       "      <th>498</th>\n",
       "      <th>499</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>4472</td>\n",
       "      <td>113</td>\n",
       "      <td>103</td>\n",
       "      <td>32</td>\n",
       "      <td>15</td>\n",
       "      <td>16</td>\n",
       "      <td>2</td>\n",
       "      <td>19</td>\n",
       "      <td>178</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>52</td>\n",
       "      <td>154</td>\n",
       "      <td>462</td>\n",
       "      <td>33</td>\n",
       "      <td>89</td>\n",
       "      <td>78</td>\n",
       "      <td>285</td>\n",
       "      <td>16</td>\n",
       "      <td>145</td>\n",
       "      <td>95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>106</td>\n",
       "      <td>607</td>\n",
       "      <td>624</td>\n",
       "      <td>35</td>\n",
       "      <td>534</td>\n",
       "      <td>6</td>\n",
       "      <td>227</td>\n",
       "      <td>7</td>\n",
       "      <td>129</td>\n",
       "      <td>113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>687</td>\n",
       "      <td>23</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>3693</td>\n",
       "      <td>42</td>\n",
       "      <td>38</td>\n",
       "      <td>39</td>\n",
       "      <td>...</td>\n",
       "      <td>26</td>\n",
       "      <td>49</td>\n",
       "      <td>2</td>\n",
       "      <td>15</td>\n",
       "      <td>566</td>\n",
       "      <td>30</td>\n",
       "      <td>579</td>\n",
       "      <td>21</td>\n",
       "      <td>64</td>\n",
       "      <td>2574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>19</td>\n",
       "      <td>14</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>226</td>\n",
       "      <td>251</td>\n",
       "      <td>7</td>\n",
       "      <td>61</td>\n",
       "      <td>113</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 500 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   0    1    2    3    4    5     6    7    8    9    ...   490  491  492  \\\n",
       "0    0    0    0    0    0    0     0    0    0    0  ...  4472  113  103   \n",
       "1    0    0    0    0    0    0     0    0    0    0  ...    52  154  462   \n",
       "2    0    0    0    0    0    0     0    0    0    0  ...   106  607  624   \n",
       "3  687   23    4    2    2    6  3693   42   38   39  ...    26   49    2   \n",
       "4    0    0    0    0    0    0     0    0    0    0  ...    19   14    5   \n",
       "\n",
       "   493  494  495  496  497  498   499  \n",
       "0   32   15   16    2   19  178    32  \n",
       "1   33   89   78  285   16  145    95  \n",
       "2   35  534    6  227    7  129   113  \n",
       "3   15  566   30  579   21   64  2574  \n",
       "4    2    6  226  251    7   61   113  \n",
       "\n",
       "[5 rows x 500 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "#print(X_train[1])\n",
    "#print(type(X_train))\n",
    "print(y_test[4])\n",
    "#print(X_train)\n",
    "tDf = pd.DataFrame(X_train)\n",
    "tDf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# truncate and pad input sequences\n",
    "max_review_length = 500\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=max_review_length)\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=max_review_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000, 500)\n",
      "[   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    1   14   20   47  111  439 3445   19\n",
      "   12   15  166   12  216  125   40    6  364  352  707 1187   39  294\n",
      "   11   22  396   13   28    8  202   12 1109   23   94    2  151  111\n",
      "  211  469    4   20   13  258  546 1104    2   12   16   38   78   33\n",
      "  211   15   12   16 2849   63   93   12    6  253  106   10   10   48\n",
      "  335  267   18    6  364 1242 1179   20   19    6 1009    7 1987  189\n",
      "    5    6    2    7 2723    2   95 1719    6    2    7 3912    2   49\n",
      "  369  120    5   28   49  253   10   10   13 1041   19   85  795   15\n",
      "    4  481    9   55   78  807    9  375    8 1167    8  794   76    7\n",
      "    4   58    5    4  816    9  243    7   43   50]\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(X_train[9])\n",
    "# print(type(X_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-11 09:33:18.673190: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2022-01-11 09:33:18.673229: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-01-11 09:33:18.673260: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (localhost.localdomain): /proc/driver/nvidia/version does not exist\n",
      "2022-01-11 09:33:18.673532: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 500, 32)           160000    \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 100)               53200     \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1)                 101       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 213,301\n",
      "Trainable params: 213,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/3\n",
      "391/391 [==============================] - 134s 338ms/step - loss: 0.4235 - accuracy: 0.7976 - val_loss: 0.3114 - val_accuracy: 0.8717\n",
      "Epoch 2/3\n",
      "391/391 [==============================] - 133s 341ms/step - loss: 0.2684 - accuracy: 0.8948 - val_loss: 0.3634 - val_accuracy: 0.8666\n",
      "Epoch 3/3\n",
      "391/391 [==============================] - 133s 340ms/step - loss: 0.2342 - accuracy: 0.9093 - val_loss: 0.2983 - val_accuracy: 0.8798\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fe6807be0d0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create the model\n",
    "embedding_vecor_length = 32\n",
    "model = Sequential()\n",
    "model.add(Embedding(x=top_words,\n",
    "                    y=embedding_vecor_length,\n",
    "                    input_length=max_review_length))\n",
    "model.add(LSTM(100))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=3, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 87.98%\n"
     ]
    }
   ],
   "source": [
    "# Final evaluation of the model\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_2 (Embedding)     (None, 500, 32)           160000    \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 500, 32)           0         \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, 100)               53200     \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 100)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 101       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 213,301\n",
      "Trainable params: 213,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/3\n",
      "391/391 [==============================] - 104s 262ms/step - loss: 0.4718 - accuracy: 0.7743\n",
      "Epoch 2/3\n",
      "391/391 [==============================] - 103s 264ms/step - loss: 0.3608 - accuracy: 0.8517\n",
      "Epoch 3/3\n",
      "391/391 [==============================] - 103s 264ms/step - loss: 0.2664 - accuracy: 0.8952\n",
      "Accuracy: 86.28%\n"
     ]
    }
   ],
   "source": [
    "# Same as above but with dropout layers added\n",
    "\n",
    "# LSTM with Dropout for sequence classification in the IMDB dataset\n",
    "\n",
    "# fix random seed for reproducibility\n",
    "numpy.random.seed(7)\n",
    "# load the dataset but only keep the top n words, zero the rest\n",
    "top_words = 5000\n",
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=top_words)\n",
    "# truncate and pad input sequences\n",
    "max_review_length = 500\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=max_review_length)\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=max_review_length)\n",
    "# create the model\n",
    "embedding_vecor_length = 32\n",
    "model = Sequential()\n",
    "model.add(Embedding(top_words, embedding_vecor_length, input_length=max_review_length))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(100))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())\n",
    "model.fit(X_train, y_train, epochs=3, batch_size=64)\n",
    "# Final evaluation of the model\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_3 (Embedding)     (None, 500, 32)           160000    \n",
      "                                                                 \n",
      " lstm_2 (LSTM)               (None, 100)               53200     \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1)                 101       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 213,301\n",
      "Trainable params: 213,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/3\n",
      "391/391 [==============================] - 144s 363ms/step - loss: 0.4658 - accuracy: 0.7733\n",
      "Epoch 2/3\n",
      "391/391 [==============================] - 142s 363ms/step - loss: 0.3229 - accuracy: 0.8682\n",
      "Epoch 3/3\n",
      "391/391 [==============================] - 141s 361ms/step - loss: 0.2758 - accuracy: 0.8855\n",
      "Accuracy: 86.51%\n"
     ]
    }
   ],
   "source": [
    "# LSTM with dropout for sequence classification in the IMDB dataset\n",
    "# Added a specific dropout on the LSTM layer instead of separate layer\n",
    "\n",
    "# fix random seed for reproducibility\n",
    "numpy.random.seed(7)\n",
    "# load the dataset but only keep the top n words, zero the rest\n",
    "top_words = 5000\n",
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=top_words)\n",
    "# truncate and pad input sequences\n",
    "max_review_length = 500\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=max_review_length)\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=max_review_length)\n",
    "# create the model\n",
    "embedding_vecor_length = 32\n",
    "model = Sequential()\n",
    "model.add(Embedding(top_words, embedding_vecor_length, input_length=max_review_length))\n",
    "model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())\n",
    "model.fit(X_train, y_train, epochs=3, batch_size=64)\n",
    "# Final evaluation of the model\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_4 (Embedding)     (None, 500, 32)           160000    \n",
      "                                                                 \n",
      " conv1d (Conv1D)             (None, 500, 32)           3104      \n",
      "                                                                 \n",
      " max_pooling1d (MaxPooling1D  (None, 250, 32)          0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " lstm_3 (LSTM)               (None, 100)               53200     \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 1)                 101       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 216,405\n",
      "Trainable params: 216,405\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/3\n",
      "391/391 [==============================] - 56s 139ms/step - loss: 0.4232 - accuracy: 0.7901\n",
      "Epoch 2/3\n",
      "391/391 [==============================] - 55s 142ms/step - loss: 0.2470 - accuracy: 0.9028\n",
      "Epoch 3/3\n",
      "391/391 [==============================] - 56s 142ms/step - loss: 0.1999 - accuracy: 0.9235\n",
      "Accuracy: 87.69%\n"
     ]
    }
   ],
   "source": [
    "#We can easily add a one-dimensional CNN and max pooling layers after \n",
    "#the Embedding layer which then feed the consolidated features to the LSTM. \n",
    "#We can use a smallish set of 32 features with a small filter \n",
    "#length of 3. The pooling layer can use the standard length of 2 \n",
    "#to halve the feature map size.\n",
    "\n",
    "# LSTM and CNN for sequence classification in the IMDB dataset\n",
    "import numpy\n",
    "from keras.datasets import imdb\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "# fix random seed for reproducibility\n",
    "numpy.random.seed(7)\n",
    "# load the dataset but only keep the top n words, zero the rest\n",
    "top_words = 5000\n",
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=top_words)\n",
    "# truncate and pad input sequences\n",
    "max_review_length = 500\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=max_review_length)\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=max_review_length)\n",
    "# create the model\n",
    "embedding_vecor_length = 32\n",
    "model = Sequential()\n",
    "model.add(Embedding(top_words, embedding_vecor_length, input_length=max_review_length))\n",
    "model.add(Conv1D(filters=32, kernel_size=3, padding='same', activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(LSTM(100))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())\n",
    "model.fit(X_train, y_train, epochs=3, batch_size=64)\n",
    "# Final evaluation of the model\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_5 (Embedding)     (None, 500, 32)           160000    \n",
      "                                                                 \n",
      " conv1d_1 (Conv1D)           (None, 500, 32)           3104      \n",
      "                                                                 \n",
      " max_pooling1d_1 (MaxPooling  (None, 250, 32)          0         \n",
      " 1D)                                                             \n",
      "                                                                 \n",
      " lstm_4 (LSTM)               (None, 100)               53200     \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 1)                 101       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 216,405\n",
      "Trainable params: 216,405\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/6\n",
      "391/391 [==============================] - 55s 139ms/step - loss: 0.4326 - accuracy: 0.7930\n",
      "Epoch 2/6\n",
      "391/391 [==============================] - 54s 139ms/step - loss: 0.2471 - accuracy: 0.9028\n",
      "Epoch 3/6\n",
      "391/391 [==============================] - 55s 140ms/step - loss: 0.2010 - accuracy: 0.9246\n",
      "Epoch 4/6\n",
      "391/391 [==============================] - 55s 140ms/step - loss: 0.1730 - accuracy: 0.9356\n",
      "Epoch 5/6\n",
      "391/391 [==============================] - 55s 140ms/step - loss: 0.1421 - accuracy: 0.9480\n",
      "Epoch 6/6\n",
      "391/391 [==============================] - 55s 140ms/step - loss: 0.1097 - accuracy: 0.9633\n",
      "Accuracy: 87.26%\n"
     ]
    }
   ],
   "source": [
    "#Same as above but added additional epochs\n",
    "\n",
    "# LSTM and CNN for sequence classification in the IMDB dataset\n",
    "import numpy\n",
    "from keras.datasets import imdb\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "# fix random seed for reproducibility\n",
    "numpy.random.seed(7)\n",
    "# load the dataset but only keep the top n words, zero the rest\n",
    "top_words = 5000\n",
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=top_words)\n",
    "# truncate and pad input sequences\n",
    "max_review_length = 500\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=max_review_length)\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=max_review_length)\n",
    "# create the model\n",
    "embedding_vecor_length = 32\n",
    "model = Sequential()\n",
    "model.add(Embedding(top_words, embedding_vecor_length, input_length=max_review_length))\n",
    "model.add(Conv1D(filters=32, kernel_size=3, padding='same', activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(LSTM(100))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())\n",
    "model.fit(X_train, y_train, epochs=6, batch_size=64)\n",
    "# Final evaluation of the model\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPZJZKgNDsjJFjiDrOCivnc",
   "collapsed_sections": [],
   "name": "_template_wkX_ML1010_.ipynb",
   "provenance": [
    {
     "file_id": "1R_J6p2OeOjXlMWk1bRbiOo7XdfzkzjGU",
     "timestamp": 1637805661262
    }
   ]
  },
  "kernelspec": {
   "display_name": "ML1010_env2",
   "language": "python",
   "name": "ml1010_env2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
